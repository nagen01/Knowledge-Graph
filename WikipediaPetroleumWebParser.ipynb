{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EyzxdJYBq-1L"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOoPB0KAq-1d"
   },
   "source": [
    "### Wiki list of American actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FG9_tviq-1g"
   },
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "BASE_URL = 'https://en.wikipedia.org'\n",
    "MAIN_URL = BASE_URL+ '/wiki/Petroleum'\n",
    "total_added = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3W82I1Jq-1p"
   },
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request(\"GET\", url)\n",
    "    return BeautifulSoup(r.data,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_soup(Final_input_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9hl0_F7q-1y"
   },
   "source": [
    "#### Text preprocessing - stop word & citation removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXGvUt4Oq-11"
   },
   "outputs": [],
   "source": [
    "# Function returns the negation handled word if it is presend in the appos dictionary\n",
    "# Else returns the word itself\n",
    "def negationHandling(word):\n",
    "    if word in appos:\n",
    "        return appos[word]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "# Check if a word is a Stopword\n",
    "# Stopword is a word that is commonly present in most of the documents and does not affect the model\n",
    "def isNotStopWord(word):\n",
    "    return word not in stopwords.words('english')\n",
    "\n",
    "\n",
    "def preprocessingText(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokens = []\n",
    "    temp = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        #Converting to LowerCase\n",
    "#         words = map(str.lower, words)\n",
    "        \n",
    "        # Remove stop words\n",
    "        words = filter(lambda x: isNotStopWord(x), words)\n",
    "        \n",
    "        # Removing punctuations except '<.>/<?>/<!>'\n",
    "        punctuations = '\"#$%&\\'()*+,-/:;<=>@\\\\^_`{|}~'\n",
    "        words = map(lambda x: x.translate(str.maketrans('', '', punctuations)), words)\n",
    "        \n",
    "        # Remove empty strings\n",
    "        words = filter(lambda x: len(x) > 0, words)\n",
    "      \n",
    "        tokens = tokens + list(words)\n",
    "        temp = ' '.join(word for word in tokens)\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzXyAloZq-2A"
   },
   "source": [
    "#### Parsing each actor webpage and writing into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t--g84vbq-2C"
   },
   "outputs": [],
   "source": [
    "# write the given content into text file with name <title>.txt\n",
    "def write_text_into_file(title, data, modified_data):\n",
    "    os.chdir(r'/home/nb01/C_Drive/Knowledge_Graph_Creation/Dataset/PetroleumDataset')\n",
    "    filename = title + \".txt\"\n",
    "    f = open(filename, 'w+', encoding=\"utf-8\")\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "    \n",
    "    os.chdir(r'/home/nb01/C_Drive/Knowledge_Graph_Creation/Dataset/PetroleumPreprocessedDataset')\n",
    "    filename = title + \".txt\"\n",
    "    f = open(filename, 'w+', encoding=\"utf-8\")\n",
    "    f.write(modified_data)\n",
    "    f.close()\n",
    "    #with open(filename, 'r') as f:\n",
    "    #    print(f)\n",
    "    #    k = f.read()\n",
    "    #    print(k)\n",
    "    print(\"Text file \" + filename + \" created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse each actor webpage content\n",
    "def parse_webpage_content(link):\n",
    "    soup = get_soup(link)\n",
    "    #results = soup.find_all(\"div\", {\"class\": \"mw-parser-output\"})[0]\n",
    "    no_of_paragraphs = 0\n",
    "    paragraphs = soup.find_all('p')\n",
    "    #print(paragraphs)\n",
    "    data = \"\"\n",
    "    for para in paragraphs:\n",
    "        #if para.id != \"mw-empty-elt\":\n",
    "        data += para.text.strip() +\"\\n\"\n",
    "        #print(data)\n",
    "        #no_of_paragraphs += 1\n",
    "        #if no_of_paragraphs == 3:\n",
    "        #    break\n",
    "            \n",
    "#   extracting 2 sentences from the paragraph\n",
    "    #data = \".\".join(data.split(\".\")[:2])\n",
    "    modified_data = preprocessingText(data+\".\") \n",
    "    return data, modified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DZCAoAIq-2O"
   },
   "source": [
    "#### Parsing all actors' content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i6V1Ny8q-2R"
   },
   "outputs": [],
   "source": [
    "# iterate through every group\n",
    "def parse_all_petroleums_from_wiki(url):\n",
    "    #name = 'petroleum'\n",
    "    #data = parse_webpage_content(url) #Getting main page data\n",
    "    #write_text_into_file(name, data) #Writting main page data\n",
    "    \n",
    "    #Processing the Pages that link to \"Petroleum\"\n",
    "    soup = get_soup(url)\n",
    "    #Getting link for petroleum linked url\n",
    "    linked_url = soup.find_all(\"div\", {\"class\":\"portal\", \"id\":\"p-tb\"})[0].find_next('ul').find_all('li')[0].a['href'].strip()\n",
    "    #print(BASE_URL + linked_url)\n",
    "    soup = get_soup(BASE_URL + linked_url)\n",
    "    #print(soup)\n",
    "    results = soup.find_all(\"div\", {\"id\":\"mw-content-text\"})\n",
    "    #print(results)\n",
    "    no_of_petroleum_links = 0\n",
    "    for res in results:\n",
    "        # iterator through every actor or <li> element\n",
    "        li_list = res.find_next('ul').find_all('li')\n",
    "        #print(li_list)\n",
    "        for li in li_list:\n",
    "            name = li.a.text.strip()\n",
    "            #print(name)\n",
    "            link = li.a['href'].strip()\n",
    "            #print(link)\n",
    "            data, modified_data = parse_webpage_content(BASE_URL+link)\n",
    "            write_text_into_file(name, data, modified_data)\n",
    "            no_of_petroleum_links += 1\n",
    "            #print(no_of_petroleum_links)\n",
    "            #if no_of_petroleum_links == 1:\n",
    "            #    break\n",
    "    print(no_of_petroleum_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKDuHfxnq-2b"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parse_all_petroleums_from_wiki(MAIN_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXfVmaZbq-2t",
    "outputId": "b1566aee-73c1-4286-dade-966cc548caec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file Alaska.txt created\n",
      "Text file Alkane.txt created\n",
      "Text file Asphalt.txt created\n",
      "Text file Economy of Angola.txt created\n",
      "Text file Alberta.txt created\n",
      "Text file Afghanistan.txt created\n",
      "Text file August 27.txt created\n",
      "Text file Economy of Azerbaijan.txt created\n",
      "Text file Geography of Azerbaijan.txt created\n",
      "Text file Acetylene.txt created\n",
      "Text file Economy of Alberta.txt created\n",
      "Text file Australian English.txt created\n",
      "Text file Airline.txt created\n",
      "Text file Andalusia.txt created\n",
      "Text file Alexandria.txt created\n",
      "Text file Alexandria, Louisiana.txt created\n",
      "Text file Black Sea.txt created\n",
      "Text file Belize.txt created\n",
      "Text file Brunei.txt created\n",
      "Text file Economy of the Bahamas.txt created\n",
      "Text file Economy of Botswana.txt created\n",
      "Text file Geography of Brazil.txt created\n",
      "Text file Economy of Brunei.txt created\n",
      "Text file Economy of Burundi.txt created\n",
      "Text file Outline of biology.txt created\n",
      "Text file Balkans.txt created\n",
      "Text file Blue whale.txt created\n",
      "Text file Geography of Canada.txt created\n",
      "Text file Economy of Canada.txt created\n",
      "Text file Transportation in Canada.txt created\n",
      "Text file Canadaâ€“United States relations.txt created\n",
      "Text file Carbon.txt created\n",
      "Text file Geography of Chad.txt created\n",
      "Text file Economy of Chad.txt created\n",
      "Text file Foreign relations of Chad.txt created\n",
      "Text file Geography of Cambodia.txt created\n",
      "Text file Cameroon.txt created\n",
      "Text file Economy of Cameroon.txt created\n",
      "Text file Geography of the Central African Republic.txt created\n",
      "Text file Economy of the Central African Republic.txt created\n",
      "Text file Foreign relations of the Central African Republic.txt created\n",
      "Text file Chad.txt created\n",
      "Text file Economy of Chile.txt created\n",
      "Text file Geography of Ivory Coast.txt created\n",
      "Text file Economy of Croatia.txt created\n",
      "Text file History of Cuba.txt created\n",
      "Text file Economy of Colombia.txt created\n",
      "Text file Carbon dioxide.txt created\n",
      "Text file Catalysis.txt created\n",
      "Text file Comet.txt created\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WikipediaActorsWebParser.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
